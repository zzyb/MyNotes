# 分布式文件系统 HDFS

​	**HDFS** （**Hadoop Distributed File System**）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。

## 一、HDFS设计

###  HDFS 架构

HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：

- **NameNode** : 负责执行有关 `文件系统命名空间` 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。
- **DataNode**：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。

### 1.2 文件系统命名空间

​	HDFS 的 `文件系统命名空间` 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但<u>不支持硬链接和软连接</u>。`NameNode` 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。

### 1.3 数据复制

由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列**块**，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。

![]()

### 1.4 数据复制的实现原理

大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，**同一机架中**的服务器间的**读取带宽**大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：

在写入程序位于 `datanode` 上时，就优先将写入文件的一个副本放置在该 `datanode` 上，否则放在随机 `datanode` 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。

![]()

如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 `（复制系数 - 1）/机架数量 + 2`，需要注意的是不允许同一个 `dataNode` 上具有同一个块的多个副本。

### 2.5 副本的选择

为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。

### 1.6 架构的稳定性

#### 1. 心跳机制和重新复制

每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。

#### 2. 数据的完整性

由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：

当客户端创建 HDFS 文件时，它会计算文件的每个块的 `校验和`，并将 `校验和` 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 `校验和` 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。

#### 3.元数据的磁盘故障

`FsImage` 和 `EditLog` 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 `FsImage` 和 `EditLog` 多副本同步，这样 `FsImage` 或 `EditLog` 的任何改变都会引起每个副本 `FsImage` 和 `EditLog` 的同步更新。

#### 4.支持快照

快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。

## 二、HDFS特点

- 超大文件存储
- 流式数据访问
- 部署在低成本硬件上
- 为高数据吞吐量而来，不适合要求极低延迟的应用
- 不适合存储大量小文件
- 只支持单个写入者，且文件只能添加数据不能修改

## HDFS基本概念



## HDFS命令行接口



## Java接口



## 数据流

### 剖析文件读取

![](./HDFS.resource/剖析文件读取.png)

1. 步骤 1：客户端通过调用**FileSyste对象**的open()方法打开希望读取的文件。
   1. 在HDFS中这个对象是<u>DistributedFileSystem的一个实例</u>。
2. 步骤 2：DistributedFileSystem通过使用远程调用(RPC)来调用namenode，以确定文件的**起始块**位置。
   1. 对于每一个块，namenode 返回存有该块复本的datanode地址。
   2. 这些datanode根据它们与客户端的距离来排序(根据几群的网络拓扑)。
   3. 如果该客户端本身就是一个datanode（比如，在一个MapReduce任务中），那么客户端将会从保存有相应数据块复本的本地datanode读取数据。
3. DistributedFileSystem 返回 一个 `FSDataInpuStream` 对象（一个支持文件定位的输入流）给客户端以便读取数据。
   1. `FSDataInputStream`类转而封装DFSInputStream对象，该对象管理着datanode和namenode的I/O。
4. 步骤 3：客户端对输入流调用read()方法。
   1. 存储着**文件起始几个块**的datanode地址的DFSInputStream 随即连接<u>距离最近的文件中的第一个块</u>所在的datanode。
   2. 步骤 4：通过对数据流反复调用read()方法，可以将数据从datanode传输到客户端。
   3. 步骤 5：到达块末端时，DFSInputStream关闭与该datanode的连接，然后寻找下一个块的最佳datanode。
   4. 所有这些操作对客户端都是透明的，在客户端看来它一直在读取一个而连续的流。
5. 步骤 6 ：客户端从流中读取数据的时候，块是按照打开DFSInputStream 与 datanode 新建连接的顺序读取的。它也会根据需要询问namenode来检索下一批数据块的datanode的位置。<u>一旦客户端完成读取，就对`FSDataInputStream`调用close()方法</u>。
6. 在读取数据的时候，如果DFSInputStream在与datanode通信时遇到错误：
   1. 会尝试从这个块的另外一个最邻近datanode读取数据。
   2. 记住故障的datanode，以保证以后不会反复读取该节点上后续的块。
7. DFSInputStream会通过校验和确认datanode发来的数据是否完整。如果发现有损坏：
   1. DFSInputStream试图从其他datanode读取其复本。
   2. 将被损坏的块通知给namenode。



**设计重点**：

- 客户端可以直接连接到datanode检索数据，并且namenode告知客户端每块所在的最佳datanode。
  - 数据流分散在集群中的所有datanode，所以这种设计能使HDFS扩展到大量的并发客户端。
- namenode只需要相应相应块位置的请求（这些数据存储在内存中，非常高效），无需响应数据请求。



### 剖析文件写入

![](./HDFS.resource/剖析文件写入.png)

1. 步骤 1：客户端通过对DistributedFileSystem对象调用create()来新建文件。
2. 步骤 2：DistributedFileSystem对namenode创建一个RPC调用，在文件系统的命名空间中新建一个文件，此时文件还没有相应的数据块。
   - namenode执行各种不同的检查以确保这个文件不存在以及客户端有新建该文件的权限。
   - 如果各项检查均通过，namenode就会为创建新文件记录一条记录；否则，文件创建失败并向客户端抛出一个IOException异常。
3. DistributedFileSystem 向客户端返回一个FSDataOutputStream 对象，由此客户端可以开始写入数据。
   - FSDataOutputStream 封装一个 `DFSoutPutstream`对象。<u>该对象</u>负责处理datanode和namenode之间的通信。
4. 步骤 3：在客户端写入数据时：
   - `DFSoutPutstream`将它们分成一个个的数据包，并写入内部队列，称为“**数据队列**（data queue）”。
   - DataStreamer 处理数据队列，它的责任是 <u>负责挑选出适合存储**数据复本**的一组datanode</u>，并据此<u>要求namenode分配新的数据块</u>。
   - 这一组datanode构成一个管线（假设复本数为3，那么管线就有3个节点）。
     - 步骤 4：DataStreamer将数据包流式传输到管线中的第一个datanode，该datanode存储数据包并将它发送到管线中第二个datanode。同样，第二个datanode存储数据包并将它发送到第三个datanode（也是最后一个）。
5. 步骤 5：`DFSoutPutstream`维护着一个<u>内部数据包队列</u>来等待datanode的收到确认回执，称为“**确认队列**（ack queue）”。
   - 收到管道中所有datanode确认消息后，该数据包才会从确认队列删除。
6. 如果任何datanode 在数据写入期间发生故障，则执行一下步骤：
   - 首先关闭管线，确认把队列中的所有数据包都添加回数据队列的最前端，以确保故障节点下游datanode不会漏掉任何一个数据包。
   - 为存储在另一个正常datanode的当前数据块指定一个新的标识，并将该标识传递给namenode，以便故障的datanode在恢复后可以删除存储的部分数据块。
   - 从管线删除故障的datanode，基于两个正常的datanode构建一条新的管线。余下的数据块写入管线中正常的datanode。
   - namenode注意到复本块不足时，会在另一个节点上创建一个新的复本。后续的数据块，继续正常接受处理。
7. 在一个块写入期间可能会有多个datanode同时发生故障，但非常少见。
   - 只要写入了dfs.namenode.replication.min的复本数（默认1），写操作就会成功，并且这个块可以在集群中异步复制，知道其目标复本数（dfs.replication-默认3）。
8. 步骤 6：客户端完成数据写入后，对数据流调用close方法。
   - 该操作:
     - 将剩余的所有数据包写入到datanode管线；
     - 步骤 7 ：并联系到namenode告知其完成文件写入前，等待确认。
9. namenode已经知道文件由哪些块组成（DataStreamer请求分配数据块），所以它在返回成功前只需要等待数据块进行最小量的复制。



### 复本怎么放

Hadoop默认策略：

- **客户端节点**上放第1个复本。
- 第2个复本放在与第一个**不同且随机**另外选择的**机架中**节点上（离架）。
- 第3个复本放在第2个复本同一个机架上，且随机选择另一个节点。
- 其他复本放在集群中随机选择节点上。（系统会尽量避免同一机架上过多复本。）

## 通过distcp并行复制

